//LAG analysis example 
 
requests 
| serialize  
| extend RequestId = toguid(customDimensions.RequestId) 
| project-away resultCode, id, itemType, operation_Name, client_Type, client_IP, operation_SyntheticSource, appId, itemId, itemCount, source, url, performanceBucket  
| sort by RequestId, timestamp desc 
| extend rn = row_number() 
| extend rncr = row_number(1, prev(RequestId,1,0) != RequestId)  
| extend previousTimestamp = iif(prev(RequestId,1,0) != RequestId, timestamp, prev(timestamp,1,0))  
| extend deltaInMin = datetime_diff('minute', previousTimestamp, timestamp) 
| project rncr, timestamp, RequestId, name, success, deltaInMin, duration, customDimensions, operation_Id, operation_ParentId, cloud_RoleInstance, appName 
 
let SampleData = datatable (user:string, rowValue: int) ["A",5,"B",12,"B",15,"A",3,"A",9,"A",19,"B",7]; 
SampleData  
| serialize 
| extend rowNumber = row_number() 
| extend rowNumberCurrentUser = row_number(1, prev(user,1,0) != user)  
| extend previousValue = strcat("Previous value was ", prev(rowValue,1,0)) 
| extend nextValue = strcat("Next value was ", next(rowNumber,1,0)) 
| extend runningTotal = row_cumsum(rowValue) 
| project rowNumber, rowNumberCurrentUser, user, rowValue, previousValue, nextValue, runningTotal  
 
// Top N by group example via LAG - option 1 
 
let SampleUserData=datatable (UserId:int, OperationDate:datetime) 
[1, datetime(2018-01-01 12:30:00), 
1, datetime(2018-01-01 13:30:00), 
2, datetime(2018-01-02 12:30:00), 
2, datetime(2018-01-03 13:30:00), 
3, datetime(2018-01-02 12:30:00), 
3, datetime(2018-01-01 12:30:00), 
3, datetime(2018-02-02 13:30:00), 
1, datetime(2018-02-02 12:30:00), 
1, datetime(2018-02-03 12:30:00), 
3, datetime(2018-03-01 12:30:00), 
3, datetime(2018-03-02 12:30:00), 
2, datetime(2018-03-02 12:30:00), 
1, datetime(2018-03-03 11:30:00), 
1, datetime(2018-03-03 12:30:00), 
1, datetime(2018-03-03 13:30:00) 
]; 
SampleUserData 
| extend MonthNum = datetime_part("Month", OperationDate)  
| summarize CountByMonthNumUserId = count() by MonthNum,UserId 
| order by MonthNum asc, UserId asc, CountByMonthNumUserId desc 
| extend RowNum = row_number(1, prev(MonthNum) != MonthNum) 
| extend CountIsSameAsPrev = CountByMonthNumUserId == prev(CountByMonthNumUserId) 
| where RowNum in (1,2) or CountIsSameAsPrev 
| project-away RowNum, CountIsSameAsPrev  
 
// Top N by Group example via top-nested - option 2 
 
let SampleUserData=datatable (UserId:int, OperationDate:datetime) 
[1, datetime(2018-01-01 12:30:00), 
1, datetime(2018-01-01 13:30:00), 
2, datetime(2018-01-02 12:30:00), 
2, datetime(2018-01-03 13:30:00), 
3, datetime(2018-01-02 12:30:00), 
3, datetime(2018-01-01 12:30:00), 
3, datetime(2018-02-02 13:30:00), 
1, datetime(2018-02-02 12:30:00), 
1, datetime(2018-02-03 12:30:00), 
3, datetime(2018-03-01 12:30:00), 
3, datetime(2018-03-02 12:30:00), 
2, datetime(2018-03-02 12:30:00), 
1, datetime(2018-03-03 11:30:00), 
1, datetime(2018-03-03 12:30:00), 
1, datetime(2018-03-03 13:30:00) 
]; 
let SampleUserData2 = 
SampleUserData 
| extend MonthNum = datetime_part("Month", OperationDate); 
SampleUserData2 
| top-nested toscalar(SampleUserData2 | summarize dcount(MonthNum)) of MonthNum by max(1), top-nested 2 of UserId by count() 
| project-away aggregated_MonthNum 
 
// Make series to fill in gaps with default for bin by bucket. What are gaps? What if you have no data for a period of time? You skip the bucket and your line graphs will lie to you. 
 
let window = 1d; 
let bucket = 1h; 
let min_t = toscalar(customMetrics | where timestamp > ago(window) | summarize min(timestamp)); 
let max_t = toscalar(customMetrics | where timestamp > ago(window) | summarize max(timestamp)); 
customMetrics 
| where timestamp > ago(window) 
| make-series totalHeartbeatCountByHour=count() default=0 on timestamp in range(min_t, max_t, bucket)  
| mvexpand timestamp to typeof(datetime), 
totalHeartbeatCountByHour to typeof(double) 
| sort by timestamp desc 
 
